{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# CUDA + ONNX Sanity\n",
    "\n",
    "This notebook checks:\n",
    "- PyTorch + CUDA availability and speed\n",
    "- Exporting a small PyTorch model to ONNX\n",
    "- Running it with ONNX Runtime (GPU if available)\n",
    "- Output parity and quick benchmarks\n",
    "\n",
    "**Prereqs** (already set up per your env):\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install onnx onnxruntime-gpu  # falls back to CPU if no GPU provider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Cell 2 — Code: versions + providers\n",
    "# python\n",
    "import os, time, pathlib, sys, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "print(\"onnx:\", onnx.__version__)\n",
    "print(\"onnxruntime:\", ort.__version__)\n",
    "print(\"ORT providers available:\", ort.get_available_providers())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    x = torch.randn(2048, 2048, device=\"cuda\")\n",
    "    y = torch.randn(2048, 2048, device=\"cuda\")\n",
    "    # Warmup\n",
    "    _ = x @ y\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    _ = x @ y\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"GPU matmul 2048x2048:\", round(time.time() - t0, 4), \"s\")\n",
    "else:\n",
    "    print(\"CUDA not available; skipping GPU sanity.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny MLP: Linear → ReLU → Linear\n",
    "class TinyMLP(torch.nn.Module):\n",
    "    def __init__(self, in_dim: int = 32, hidden: int = 64, out_dim: int = 10) -> None:\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "in_dim, hidden, out_dim = 32, 64, 10\n",
    "model = TinyMLP(in_dim, hidden, out_dim).eval()\n",
    "\n",
    "# Random test input\n",
    "np.random.seed(0)\n",
    "x_np = np.random.randn(128, in_dim).astype(\"float32\")  # batch=128\n",
    "x_t = torch.from_numpy(x_np)\n",
    "\n",
    "# Prefer GPU for PyTorch run if available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "x_t_dev = x_t.to(device)\n",
    "\n",
    "# Torch forward\n",
    "with torch.inference_mode():\n",
    "    y_t = model(x_t_dev).cpu().numpy()\n",
    "\n",
    "y_t[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = pathlib.Path(\"models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "onnx_path = models_dir / \"tiny_mlp.onnx\"\n",
    "\n",
    "dummy = torch.randn(1, in_dim, device=device)  # shape for tracing\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy,\n",
    "    onnx_path.as_posix(),\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch\"}, \"output\": {0: \"batch\"}},\n",
    "    opset_version=17,\n",
    ")\n",
    "print(\"Exported:\", onnx_path.as_posix())\n",
    "\n",
    "# Sanity: structural check\n",
    "onnx_model = onnx.load(onnx_path.as_posix())\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model structure OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefer CUDA provider if present; else fall back to CPU\n",
    "providers = ort.get_available_providers()\n",
    "if \"CUDAExecutionProvider\" in providers:\n",
    "    sess_providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "else:\n",
    "    sess_providers = [\"CPUExecutionProvider\"]\n",
    "\n",
    "sess = ort.InferenceSession(onnx_path.as_posix(), providers=sess_providers)\n",
    "print(\"Using providers:\", sess.get_providers())\n",
    "\n",
    "# Run ORT\n",
    "ort_inputs = {\"input\": x_np}\n",
    "y_ort = sess.run([\"output\"], ort_inputs)[0]\n",
    "\n",
    "# Compare numerically: ONNX vs PyTorch\n",
    "mae = np.max(np.abs(y_ort - y_t))\n",
    "print(\"Max |ORT - Torch|:\", float(mae))\n",
    "print(\"Close? (rtol=1e-4, atol=1e-4):\", np.allclose(y_ort, y_t, rtol=1e-4, atol=1e-4))\n",
    "y_ort[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "def bench(fn: Callable[[], None], warmup: int = 5, iters: int = 20) -> float:\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        fn()\n",
    "    t0 = time.time()\n",
    "    for _ in range(iters):\n",
    "        fn()\n",
    "    return (time.time() - t0) / iters\n",
    "\n",
    "results: dict[str, float] = {}\n",
    "\n",
    "# Torch (GPU if available, else CPU)\n",
    "if torch.cuda.is_available():\n",
    "    def run_torch() -> None:\n",
    "        with torch.inference_mode():\n",
    "            _ = model(x_t_dev)\n",
    "        torch.cuda.synchronize()\n",
    "    results[\"torch(gpu)\"] = bench(run_torch)\n",
    "else:\n",
    "    def run_torch() -> None:\n",
    "        with torch.inference_mode():\n",
    "            _ = model(x_t)\n",
    "    results[\"torch(cpu)\"] = bench(run_torch)\n",
    "\n",
    "# ORT GPU (if available)\n",
    "if \"CUDAExecutionProvider\" in sess.get_providers():\n",
    "    def run_ort_gpu() -> None:\n",
    "        _ = sess.run([\"output\"], {\"input\": x_np})\n",
    "    results[\"ort(gpu)\"] = bench(run_ort_gpu)\n",
    "\n",
    "# ORT CPU\n",
    "sess_cpu = ort.InferenceSession(onnx_path.as_posix(), providers=[\"CPUExecutionProvider\"])\n",
    "def run_ort_cpu() -> None:\n",
    "    _ = sess_cpu.run([\"output\"], {\"input\": x_np})\n",
    "results[\"ort(cpu)\"] = bench(run_ort_cpu)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Notes & Troubleshooting\n",
    "\n",
    "- **Different numbers between Torch and ORT?**  \n",
    "  - Ensure `model.eval()` and `torch.inference_mode()` are used.  \n",
    "  - Export with a recent `opset_version` (we used 17).  \n",
    "  - Minor diffs (`~1e-4`) are normal due to kernels/precision.\n",
    "\n",
    "- **No `CUDAExecutionProvider` in ORT?**  \n",
    "  - Make sure you installed `onnxruntime-gpu` (not just `onnxruntime`).  \n",
    "  - Confirm your NVIDIA driver + CUDA runtime are present.\n",
    "\n",
    "- **Bigger models**  \n",
    "  - This MLP is trivial. For CNN/Transformers, export the same way.  \n",
    "  - You can also try `onnxruntime-gpu` graph optimizations or TensorRT (later).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
